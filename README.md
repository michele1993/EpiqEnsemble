# Epiq Ensemble, epistemic uncertainty in Q-function. 

**Overview**

Here, I use an ensemble approach to approximate a distribution over value fuctions as well as transition models to get an estimate of the epistemic uncertainty over value predictions.
Since RL algorithms are heavily affected by out-of-distribution transitions, achieving good uncertainty estimates over value predictions can greatly mitigate this problem. 

**EpiqEnsemble**

Here, I implement a model-based ensemble version of the [TD3](https://arxiv.org/abs/1802.09477) algorithm. 
Specifially, I adopt an ensemble architecture for both the critic and the transition model, pairing each network in the critic ensemble with a network in the transition model ensemble.
The figure below provides an example of how the pairing looks like for critic and transition ensembles with 3 networks each.

<img src="https://github.com/michele1993/EpiqEnsemble/blob/master/img/Epiq_ensemble.png" alt="Figure: Ensemble" width="60%" height="60%">

Next, I train the critic ensemble based on a [Dyna](https://www.sciencedirect.com/science/article/abs/pii/B9781558601413500304) approach.
Specifically, I train each critic network with the 'synthetic' one step transitions generated by the paired network in the transition ensemble. 
Crucially, each network in the transition model ensemble should predict a slightly different transition given the same state-action pair, which will then be reflected on how each (paired) critic network is updated.
As a result, the critic ensemble should come to reflect the model uncertainty over the distribution of transitions, instead of only the uncertainty over the critic predictions.

Once I have an estimate of the epistemic uncertinty over the values, I can use that estimate to drive better policy updates.
For instance, one of simplest way is to 'optimism in the face of uncertainy', where we encourage the policy to go towards areas with high value uncertainty,

$$
\nabla_\theta \pi_\theta = \nabla_\theta \mathbb{E}_i[Q_i(s, a)] + \lambda \ \sqrt{Var_i(Q_i(s, a))}
$$

here, the expectation and variance are taken over the distribution of critic ensembles, $Q$, while $\lambda$ denotes a fixed weight we cause to control the uncertainty bonus.
Alternatively, in a setting where we want to reduce risky behaviours, we can use the opposite approach (i.e., 'pessimism in the face of uncertainy'),

$$
\nabla_\theta \pi_\theta = \nabla_\theta \mathbb{E}_i[Q_i(s, a)] - \lambda \ \sqrt{Var_i(Q_i(s, a))}
$$

For instance, this latter approach should help discourage out-of-distribution actions, which may lead to catastrophic failures. 

There are a lot of other ways in which we could try to exploit the epistemic uncertainty over values to drive better behaviour. Therefore, it vital to indetify good approach for uncertainty estimation in RL settings.


